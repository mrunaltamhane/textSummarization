{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d45d9c-b7bd-4c94-9042-c52bad8368a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\mruna\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\mruna\\anaconda3\\lib\\site-packages (from PyPDF2) (4.3.0)\n",
      "Requirement already satisfied: docx2txt in c:\\users\\mruna\\anaconda3\\lib\\site-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453b852f-b26f-4ff9-a020-9c955ee62118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import docx2txt\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import networkx as nx\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a4b262-bd38-40de-8a02-b336370bce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13563ae4-aca8-4764-bbe9-88fe6e48f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9767abec-5212-474e-91cb-301b70e66155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_doc():\n",
    "    name = input('Please input a file name: ')\n",
    "    print(f'You have asked for the document {name}')\n",
    "\n",
    "    # now read the type of document\n",
    "    if name.lower().endswith('.txt'):\n",
    "        choice = 1\n",
    "    elif name.lower().endswith('.pdf'):\n",
    "        choice = 2\n",
    "    else:\n",
    "        choice = 3\n",
    "        # print(name)\n",
    "    print(choice)\n",
    "    \n",
    "    # Case 1: if it is a .txt file\n",
    "    if choice == 1:\n",
    "        with open(name, 'r') as file:\n",
    "            document = file.read()\n",
    "    \n",
    "    # Case 2: if it is a .pdf file\n",
    "    elif choice == 2:\n",
    "        pdf_reader = PdfReader(name)\n",
    "        page_obj = pdf_reader.pages[0]\n",
    "        document = page_obj.extract_text()\n",
    "    \n",
    "    # Case 3: none of the formats\n",
    "    else:\n",
    "        print('Failed to load a valid file')\n",
    "        print('Returning an empty string')\n",
    "        document = ''\n",
    "    \n",
    "    print(type(document))\n",
    "    return document\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac92371b-1d38-43bc-978a-fbfbcbe87e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summarize(text,per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fca6a2f-ccd5-4906-bb2d-bfcf69615686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "import networkx as nx\n",
    "\n",
    "def extractive_summarize(text):\n",
    "    def tokenize(document):\n",
    "        doc_tokenizer = PunktSentenceTokenizer()\n",
    "        return doc_tokenizer.tokenize(document)\n",
    "    \n",
    "    def process_document(document):\n",
    "        sentences_list = tokenize(document)\n",
    "        cv = CountVectorizer()\n",
    "        cv_matrix = cv.fit_transform(sentences_list)\n",
    "        normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
    "        res_graph = normal_matrix * normal_matrix.T\n",
    "        nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
    "        ranks = nx.pagerank(nx_graph)\n",
    "        sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
    "        sentence_array = np.asarray(sentence_array)\n",
    "        rank_max = float(sentence_array[0][0])\n",
    "        rank_min = float(sentence_array[len(sentence_array) - 1][0])\n",
    "        temp_array = []\n",
    "        \n",
    "        flag = 0\n",
    "        if rank_max - rank_min == 0:\n",
    "            temp_array.append(0)\n",
    "            flag = 1\n",
    "        \n",
    "        if flag != 1:\n",
    "            for i in range(0, len(sentence_array)):\n",
    "                temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
    "        \n",
    "        threshold = (sum(temp_array) / len(temp_array)) + 0.2\n",
    "        sentence_list = []\n",
    "        if len(temp_array) > 1:\n",
    "            for i in range(0, len(temp_array)):\n",
    "                if temp_array[i] > threshold:\n",
    "                        sentence_list.append(sentence_array[i][1])\n",
    "        else:\n",
    "            sentence_list.append(sentence_array[0][1])\n",
    "        \n",
    "        summary = \" \".join(str(x) for x in sentence_list)\n",
    "        return summary\n",
    "    \n",
    "    summary = process_document(text)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1cf46b6-c99c-4984-8769-cb1e9e9ccedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a summarization method:\n",
      "1. Abstractive Summarization\n",
      "2. Extractive Summarization\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  2\n",
      "Please input a file name:  3.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have asked for the document 3.pdf\n",
      "2\n",
      "<class 'str'>\n",
      "\n",
      "Extractive Summary:\n",
      "Under no c ircumstances \n",
      "shall McGraw-Hill and/or its licensors be liable for any indirect, incidental, special, punitive, consequential or similar damages that result from the use of or inability to use the work, even if any of them has been advised of the possibility of such damages. However, because of the possibility of human or \n",
      "mechanical error by our sources, McGraw-Hill, or others, McGraw-Hill does not guarantee the accuracy, adequacy, or completeness of any information and is not responsible for any errors or omissions or the results obtained from the use of such information. THE WORK IS PROVIDED “AS IS.” McGRAW-HILL AND ITS LICENSORS MAKE NO GUARANTEES OR WARRANTIES AS \n",
      "TO THE ACCURACY , ADEQUACY OR COMPLETENESS OF OR RESULTS TO BE OBTAINED FROM USING THE WORK, INCLUDING ANY INFORMATION THAT CAN BE ACCESSED THROUGH THE WORK VIA HYPERLINK OR OTHERWISE, AND EXPRESSLY DISCLAIM ANY WARRANTY , EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO IMPLIED WARRANTIES OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Neither McGraw-Hill nor its licensors shall be liable to you or anyone else for any inaccuracy, error or omission, regardless of cause, in the work or for any damages resulting therefrom. Except as permitted under the United States Copyright Act of 1976, no \n",
      "part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of the publisher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mruna\\AppData\\Local\\Temp\\ipykernel_161948\\728693297.py:17: DeprecationWarning: \n",
      "\n",
      "The scipy.sparse array containers will be used instead of matrices\n",
      "in Networkx 3.0. Use `from_scipy_sparse_array` instead.\n",
      "  nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"Choose a summarization method:\")\n",
    "    print(\"1. Abstractive Summarization\")\n",
    "    print(\"2. Extractive Summarization\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "    if choice == '1':\n",
    "        text = read_doc()\n",
    "        summary = abstractive_summarize(text,0.09)\n",
    "        print(\"\\nAbstractive Summary:\")\n",
    "        print(summary)\n",
    "    elif choice == '2':\n",
    "        text = read_doc()\n",
    "        summary = extractive_summarize(text)\n",
    "        print(\"\\nExtractive Summary:\")\n",
    "        print(summary)\n",
    "    else:\n",
    "        print(\"Invalid choice. Please enter 1 or 2.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f7f32e-b2a2-4959-85f5-5da3552f660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a19c3-7190-4e44-be30-c6c6a8025664",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
